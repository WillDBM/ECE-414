{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment XX: Markov Decision Process\n",
        "\n",
        "Markov Decision Process (MDP) is a mathematical tool to model and solve a sequential decision-making problem.\n",
        "\n",
        "Every RL task has an MDP: An RL agent interacts with the environment through its actions and the environment responds to the agent with a reward signal and transits the agent from the current state to the next state. The agent-environment interaction leads to the following trajectory.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/poudel-bibek/AI-Assignments/main/Images_videos/MDP/1.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 1: Agent-Environment interaction trajectory. S=State, A=Action, R=reward.</em>\n",
        "</p>\n",
        "\n",
        "\n",
        "As an example, let's go through the following Grid World, where an agent starts at state S1 and tries to reach the goal state S10 as fast as possible. The black blocks are obstacles.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/poudel-bibek/AI-Assignments/main/Images_videos/MDP/2.jpg\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 2: Grid World with 10 states. </em>\n",
        "</p>\n",
        "\n",
        "\n",
        "Here is the MDP of the Grid World.\n",
        "\n",
        "- __States:__ agent's current position\n",
        "- __Actions:__ LEFT, RIGHT, DOWN, UP. Note that not all states allow successful executions of all the actions, e.g., S1 only allows RIGHT and DOWN.\n",
        "- __Transition function:__ defines how the environment responses to the agent's actions. This grid-world has a deterministic transition function.\n",
        "- __Reward function:__ The agent receives a negative reward (penalty) of -1 at every time step (a time sensitive task). Thus, the goal of the agent is to receive as low penalty as possible.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/poudel-bibek/AI-Assignments/main/Images_videos/MDP/3.jpg\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 3: Transition function of Grid World. </em>\n",
        "</p>\n",
        "\n",
        "Now, let's set-up this MDP using [OpenAI Gym](https://gymnasium.farama.org/). Please carefully study the code and make sure you understand it.\n",
        "\n"
      ],
      "metadata": {
        "id": "5Xab2hENaUpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "txgakInlWD2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3376b6a2-c50a-4bbb-ee2d-5ed0887d7f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        }
      ],
      "source": [
        "# Legacy gym\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rNjyqM60MNJa"
      },
      "outputs": [],
      "source": [
        "class GridWorld(gym.Env):\n",
        "  def __init__(self, ):\n",
        "\n",
        "    # A simple reward function\n",
        "    self.step_reward = -1 # Every time-step incurs a -1 reward\n",
        "    self.obstacle_reward = -10 # Obstacles have a high negative reward -10\n",
        "    self.goal_reward = 10 # Goal state has a high positive reward\n",
        "\n",
        "    # Set the boundary of the grid\n",
        "    self.x_min = 0\n",
        "    self.y_min = 0\n",
        "    self.x_max = 3 # 0, 1, 2, 3\n",
        "    self.y_max = 2 # 0, 1, 2\n",
        "\n",
        "    # Action space\n",
        "    self.action_space = gym.spaces.Discrete(4)\n",
        "    self.state_space = [(i,j) for i in range(self.x_max+1) for j in range(self.y_max+1)]\n",
        "\n",
        "    # Special places in state space\n",
        "    self.obstacle_locations = [(2,1),(3,1)]\n",
        "    self.goal_location = [(self.x_max, self.y_max)]\n",
        "    self.start_location = (self.x_min, self.y_min)\n",
        "\n",
        "    # Transition function\n",
        "    self.transition_function = self.create_transition_function()\n",
        "\n",
        "  def create_transition_function(self, ):\n",
        "    # Deterministic transition function, rewards included\n",
        "    transitions = {}\n",
        "    for state in self.state_space:\n",
        "      for action in range(self.action_space.n):\n",
        "        x, y = state\n",
        "        # Move up\n",
        "        if action == 0:\n",
        "          y = max(self.y_min, y-1)\n",
        "\n",
        "        # Move down\n",
        "        elif action == 1:\n",
        "          y = min(self.y_max, y+1)\n",
        "\n",
        "        # Move Left\n",
        "        elif action == 2:\n",
        "          x = max(x-1, self.x_min)\n",
        "\n",
        "        # Move right\n",
        "        elif action == 3:\n",
        "          x = min(x+1, self.x_max)\n",
        "\n",
        "        next_state = (x,y)\n",
        "        reward = self.step_reward\n",
        "\n",
        "        # Reward based on if we transition to special states\n",
        "        if next_state in self.obstacle_locations:\n",
        "          reward += self.obstacle_reward\n",
        "\n",
        "        elif next_state in self.goal_location:\n",
        "          reward += self.goal_reward\n",
        "\n",
        "        transitions[(state, action)] = next_state, reward\n",
        "    return transitions\n",
        "\n",
        "  def step(self, agent_location, action):\n",
        "    # Current location coordinates\n",
        "    (x,y) = agent_location\n",
        "\n",
        "    # Apply the action\n",
        "    next_location, reward = self.transition_function.get(((x,y),action))\n",
        "    #print(f\"Current Location = {agent_location} Next location = {next_location}, Reward = {reward}\")\n",
        "\n",
        "    done = self.check_terminal(next_location) # Check whether the next state is terminal or not\n",
        "\n",
        "    return next_location, reward, done, {}\n",
        "\n",
        "  def reset(self, ):\n",
        "    return self.start_location\n",
        "\n",
        "  def check_terminal(self, state):\n",
        "    # What to make the terminal states?\n",
        "    # Both goal and obstacle states are terminal because there is no notion of `future rewards' from them\n",
        "    if state in [(self.x_max, self.y_max)]:\n",
        "      return True\n",
        "\n",
        "    elif state in self.obstacle_locations:\n",
        "      return True\n",
        "\n",
        "    else:\n",
        "      return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAz0AXxMcmVo"
      },
      "source": [
        "\n",
        "---\n",
        "# Exercise 1.\n",
        "\n",
        "Please finish the following task.\n",
        "\n",
        "Write code to advance the environment step by step. Output the current state, reward, and the next state of the agent taking the following steps: RIGHT, DOWN, RIGHT. Also output a flag (in binary) to indiciate whether the current episode is terminated or not (for this problem an episode will terminate if the agent either hits an obstacle or reaches the goal state)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################ WRITE YOUR CODE HERE ################\n",
        "steps = [3,1,3]\n",
        "terminator_flag = False # False not terminated, True terminated\n",
        "\n",
        "env = GridWorld()\n",
        "location = env.reset()\n",
        "\n",
        "for step in steps:\n",
        "  next_location, reward, terminator_flag, info = env.step(location, step)\n",
        "  print(\"State: \", location)\n",
        "  print(\"Reward: \", reward)\n",
        "  print(\"Next State: \", next_location)\n",
        "  if terminator_flag:\n",
        "    print(1)\n",
        "  else:\n",
        "    print(0)\n",
        "  location = next_location\n"
      ],
      "metadata": {
        "id": "4i5XhXlImImb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a97b5d-fd91-4a3f-f78e-816ea5d5d7e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State:  (0, 0)\n",
            "Reward:  -1\n",
            "Next State:  (1, 0)\n",
            "0\n",
            "State:  (1, 0)\n",
            "Reward:  -1\n",
            "Next State:  (1, 1)\n",
            "0\n",
            "State:  (1, 1)\n",
            "Reward:  -11\n",
            "Next State:  (2, 1)\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUla56I5MMBU"
      },
      "source": [
        "---\n",
        "# Exercise 2.\n",
        "\n",
        "Please finish the following task.\n",
        "\n",
        "Write a complete MDP for a simpler version of the Grid World below.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/poudel-bibek/AI-Assignments/main/Images_videos/MDP/4.jpg\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 4: A Grid World with 7 states.</em>\n",
        "</p>\n",
        "\n",
        "\n",
        "You need to inherit the `gym.Env` class and implement the five functions given below.\n",
        "\n",
        "- __init__\n",
        "  - Define a reward function (step reward, goal reward, obstacle reward)\n",
        "  - Define action space and state space of the environment\n",
        "  - Define the grid boundary, location of the obstacle and goal states\n",
        "\n",
        "- __create_transition_function__\n",
        "  - Define what (State, Action) combination leads to what (Next State, Reward).\n",
        "  - Establishe the coordinate system for the environment\n",
        "\n",
        "- __step__\n",
        "  - Advance the environrment by one step including:\n",
        "    - Update the agent's location\n",
        "    - Return the next state and reward for the last action taken\n",
        "    - Check if the agent has reached a terminal state\n",
        "\n",
        "- __reset__\n",
        "  - Reset the agent's location (to S1) and return it\n",
        "\n",
        "- __check_terminal__\n",
        "  - Define terminal states\n",
        "  - Returns True if a given state is a terminal state\n",
        "\n",
        "\n",
        "\n",
        "You can use the code skeleton below to get started.\n",
        "\n",
        "```\n",
        "class YourOwnGridWorld(gym.Env):\n",
        "  def __init__(self, ):\n",
        "    ###      YOUR CODE HERE       ###\n",
        "    pass\n",
        "\n",
        "  def create_transition_function(self, ):\n",
        "    ###      YOUR CODE HERE       ###\n",
        "    pass\n",
        "\n",
        "  def step(self, agent_location, action):\n",
        "    ###      YOUR CODE HERE       ###\n",
        "    pass\n",
        "\n",
        "  def reset(self, ):\n",
        "    ###      YOUR CODE HERE       ###\n",
        "    pass\n",
        "\n",
        "  def check_terminal(self, state):\n",
        "    ###      YOUR CODE HERE       ###\n",
        "    pass\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class YourOwnGridWorld(gym.Env):\n",
        "  def __init__(self, ):\n",
        "\n",
        "    # A simple reward function\n",
        "    self.step_reward = -1 # Every time-step incurs a -1 reward\n",
        "    self.obstacle_reward = -10 # Obstacles have a high negative reward -10\n",
        "    self.goal_reward = 10 # Goal state has a high positive reward\n",
        "\n",
        "    # Set the boundary of the grid\n",
        "    self.x_min = 0\n",
        "    self.y_min = 0\n",
        "    self.x_max = 2 # 0, 1, 2, 3\n",
        "    self.y_max = 2 # 0, 1, 2\n",
        "\n",
        "    # Action space\n",
        "    self.action_space = gym.spaces.Discrete(4)\n",
        "    self.state_space = [(i,j) for i in range(self.x_max+1) for j in range(self.y_max+1)]\n",
        "\n",
        "    # Special places in state space\n",
        "    self.obstacle_locations = [(0,1),(0,2)]\n",
        "    self.goal_location = [(self.x_max, self.y_max)]\n",
        "    self.start_location = (self.x_min, self.y_min)\n",
        "\n",
        "    # Transition function\n",
        "    self.transition_function = self.create_transition_function()\n",
        "\n",
        "  def create_transition_function(self, ):\n",
        "    # Deterministic transition function, rewards included\n",
        "    transitions = {}\n",
        "    for state in self.state_space:\n",
        "      for action in range(self.action_space.n):\n",
        "        x, y = state\n",
        "        # Move up\n",
        "        if action == 0:\n",
        "          y = max(self.y_min, y-1)\n",
        "\n",
        "        # Move down\n",
        "        elif action == 1:\n",
        "          y = min(self.y_max, y+1)\n",
        "\n",
        "        # Move Left\n",
        "        elif action == 2:\n",
        "          x = max(x-1, self.x_min)\n",
        "\n",
        "        # Move right\n",
        "        elif action == 3:\n",
        "          x = min(x+1, self.x_max)\n",
        "\n",
        "        next_state = (x,y)\n",
        "        reward = self.step_reward\n",
        "\n",
        "        # Reward based on if we transition to special states\n",
        "        if next_state in self.obstacle_locations:\n",
        "          reward += self.obstacle_reward\n",
        "\n",
        "        elif next_state in self.goal_location:\n",
        "          reward += self.goal_reward\n",
        "\n",
        "        transitions[(state, action)] = next_state, reward\n",
        "    return transitions\n",
        "\n",
        "  def step(self, agent_location, action):\n",
        "    # Current location coordinates\n",
        "    (x,y) = agent_location\n",
        "\n",
        "    # Apply the action\n",
        "    next_location, reward = self.transition_function.get(((x,y),action))\n",
        "    #print(f\"Current Location = {agent_location} Next location = {next_location}, Reward = {reward}\")\n",
        "\n",
        "    done = self.check_terminal(next_location) # Check whether the next state is terminal or not\n",
        "\n",
        "    return next_location, reward, done, {}\n",
        "\n",
        "  def reset(self, ):\n",
        "    return self.start_location\n",
        "\n",
        "  def check_terminal(self, state):\n",
        "    # What to make the terminal states?\n",
        "    # Both goal and obstacle states are terminal because there is no notion of `future rewards' from them\n",
        "    if state in [(self.x_max, self.y_max)]:\n",
        "      return True\n",
        "\n",
        "    elif state in self.obstacle_locations:\n",
        "      return True\n",
        "\n",
        "    else:\n",
        "      return False"
      ],
      "metadata": {
        "id": "tkyyD4R2ifHy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Your Solution\n",
        "\n",
        "From Figure 4, we know that the shortest path from the starting state to the goal state is <S1, S2, S3, S5, S7>. The following code snippet, which traces the shortest path trajectory, can be used to test your MDP design.\n",
        "\n",
        "Note that you need to replace ACTION RIGHT and ACTION DOWN with the numbers you assigned for these two actions in your MDP.\n"
      ],
      "metadata": {
        "id": "jxqhcBwenLIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_env = YourOwnGridWorld()\n",
        "\n",
        "initial_observation = your_env.reset()\n",
        "print(initial_observation)\n",
        "\n",
        "# Take a step Right\n",
        "first_step, _, _, _ = your_env.step(initial_observation, 3)\n",
        "print(first_step)\n",
        "\n",
        "# Take another step Right\n",
        "second_step, _, _, _ = your_env.step(first_step, 3)\n",
        "print(second_step)\n",
        "\n",
        "# Take a step Down\n",
        "third_step, _, _, _ = your_env.step(second_step, 1)\n",
        "print(third_step)\n",
        "\n",
        "# Take another step Down\n",
        "fourth_step, _, _, _ = your_env.step(third_step, 1)\n",
        "print(fourth_step)"
      ],
      "metadata": {
        "id": "xgIqbtw7iSJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7fb1aac-4fc8-4ec5-e45c-902084ee718b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0)\n",
            "(1, 0)\n",
            "(2, 0)\n",
            "(2, 1)\n",
            "(2, 2)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}